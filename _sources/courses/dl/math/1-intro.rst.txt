### บทนำ

ส่วนใหญ่เราเห็นแคลคูลัสครั้งสุดท้ายในโรงเรียน แต่อนุพันธ์เป็นส่วนสำคัญของการเรียนรู้ของเครื่อง โดยเฉพาะอย่างยิ่งในเครือข่ายประสาทลึกที่ฝึกโดยการปรับฟังก์ชันการสูญเสีย หากคุณอ่านเอกสารการเรียนรู้ของเครื่องหรือการเขียนโปรแกรมของไลบรารีอย่าง PyTorch แคลคูลัสจะกลับเข้ามาในชีวิตของคุณอีกครั้ง และไม่ใช่แคลคูลัสแบบเก่าธรรมดา ๆ ที่ปรากฏขึ้น แต่คุณต้องการแคลคูลัสเมทริกซ์ซึ่งเป็นการรวมกันของพีชคณิตเชิงเส้นและแคลคูลัสหลายตัวแปร

แม้ว่าในความเป็นจริงแล้ว คุณอาจไม่จำเป็นต้องใช้แคลคูลัสเมทริกซ์ในระดับสูงมากนัก เนื่องจากในหลักสูตรของ Jeremy แสดงให้เห็นว่าคุณสามารถกลายเป็นผู้เชี่ยวชาญด้านการเรียนรู้เชิงลึกได้โดยใช้แคลคูลัสแบบสเกลาร์ที่มีเพียงเล็กน้อย ขอบคุณที่มีการสร้างความแตกต่างอัตโนมัติในไลบรารีการเรียนรู้เชิงลึกสมัยใหม่ แต่ถ้าคุณต้องการเข้าใจลึกซึ้งถึงสิ่งที่เกิดขึ้นในไลบรารีเหล่านี้ และเข้าใจบทความวิชาการที่พูดถึงเทคนิคการฝึกแบบใหม่ คุณจะต้องเข้าใจแคลคูลัสเมทริกซ์บางส่วน

ตัวอย่างเช่น การเปิดใช้งานของหน่วยคำนวณเดียวในเครือข่ายประสาทมักจะคำนวณโดยใช้ดอทโปรดักต์ของเวกเตอร์น้ำหนัก \( \mathbf{w} \) กับเวกเตอร์อินพุต \( \mathbf{x} \) บวกกับสเกลาร์ไบแอส \( b \): \( z(\mathbf{x}) = \sum_{i} w_i x_i + b = \mathbf{w} \cdot \mathbf{x} + b \) ฟังก์ชัน \( z(\mathbf{x}) \) นี้เรียกว่าฟังก์ชันแอฟฟินของหน่วยและตามด้วยหน่วยเชิงเส้นที่ปรับค่า ซึ่งจะตัดค่าลบเป็นศูนย์: \( \max(0, z(\mathbf{x})) \) หน่วยคำนวณนี้บางครั้งเรียกว่า "นิวรอนเทียม" และมีลักษณะดังนี้:

เครือข่ายประสาทประกอบด้วยหน่วยเหล่านี้จำนวนมาก จัดเป็นเลเยอร์หลายชั้น การเปิดใช้งานของหน่วยในเลเยอร์หนึ่งจะกลายเป็นอินพุตสำหรับหน่วยในเลเยอร์ถัดไป การเปิดใช้งานของหน่วยในเลเยอร์สุดท้ายเรียกว่าผลลัพธ์ของเครือข่าย

การฝึกนิวรอนนี้หมายถึงการเลือกน้ำหนัก \( \mathbf{w} \) และไบแอส \( b \) เพื่อให้เราได้ผลลัพธ์ที่ต้องการสำหรับอินพุตทั้งหมด \( \mathbf{x} \) เพื่อทำเช่นนั้น เราจะลดฟังก์ชันการสูญเสียที่เปรียบเทียบการเปิดใช้งานสุดท้ายของเครือข่าย \( \text{activation}(\mathbf{x}) \) กับเป้าหมาย \( \text{target}(\mathbf{x}) \) สำหรับอินพุตทั้งหมด \( \mathbf{x} \) การลดฟังก์ชันการสูญเสียใช้วิธีต่าง ๆ ของการไล่ระดับเช่น SGD, SGD with momentum หรือ Adam ทั้งหมดนี้ต้องการอนุพันธ์บางส่วน (การไล่ระดับ) ของ \( \text{activation}(\mathbf{x}) \) กับพารามิเตอร์ของโมเดล \( \mathbf{w} \) และ \( b \) เป้าหมายของเราคือการปรับแต่ง \( \mathbf{w} \) และ \( b \) ทีละเล็กทีละน้อยเพื่อให้ฟังก์ชันการสูญเสียโดยรวมลดลงในอินพุตทั้งหมด \( \mathbf{x} \)

หากเราระมัดระวัง เราสามารถหาการไล่ระดับโดยการหาอนุพันธ์ของฟังก์ชันการสูญเสียที่ใช้กันทั่วไป (เช่นค่าเฉลี่ยกำลังสองของข้อผิดพลาด):

\[ \frac{1}{N} \sum_{\mathbf{x}} (\text{target}(\mathbf{x}) - \text{activation}(\mathbf{x}))^2 = \frac{1}{N} \sum_{\mathbf{x}} (\text{target}(\mathbf{x}) - \max(0, \sum_{i} w_i x_i + b))^2 \]

แต่นี่เป็นเพียงนิวรอนเดียว และเครือข่ายประสาทต้องฝึกน้ำหนักและไบแอสของนิวรอนทั้งหมดในทุกเลเยอร์พร้อมกัน เพราะมีหลายอินพุตและผลลัพธ์ของเครือข่ายอาจมีหลายตัว เราจำเป็นต้องมีกฎทั่วไปสำหรับอนุพันธ์ของฟังก์ชันกับเวกเตอร์และกฎสำหรับอนุพันธ์ของฟังก์ชันเวกเตอร์กับเวกเตอร์ด้วย

บทความนี้จะอธิบายการหาอนุพันธ์บางส่วนที่สำคัญสำหรับการฝึกเครือข่ายประสาทสังเคราะห์ เราจะนำเสนอแคลคูลัสเมทริกซ์ซึ่งจำเป็นต่อการฝึกเครือข่ายประสาทสังเคราะห์ การเรียนรู้แคลคูลัสเมทริกซ์ไม่ยากอย่างที่คิด มีกฎเพียงไม่กี่ข้อสำคัญที่เราต้องเข้าใจ บทความสั้น ๆ นี้จะช่วยให้คุณเริ่มต้นในโลกของแคลคูลัสเมทริกซ์ที่เกี่ยวข้องกับการฝึกเครือข่ายประสาทสังเคราะห์อย่างรวดเร็ว
